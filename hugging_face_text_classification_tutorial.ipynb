{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGe1PQ-yn6D9"
   },
   "source": [
    "# HUGGING FACE TEXT CLASSIFICATION\n",
    "\n",
    "* RESOURCE NOTE BOOK : https://www.learnhuggingface.com/notebooks/hugging_face_text_classification_tutorial\n",
    "* SETUP STEPS: https://www.learnhuggingface.com/extras/setup\n",
    "\n",
    "NOTE : A GPU IS NEEDED ON GOOGLE COLAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gtywqam0JBEo"
   },
   "source": [
    "## IMPORTNG NECESSARY LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kB8LsTa3gczD",
    "outputId": "dd786d98-4b6e-4e27-adc5-2552ed4940ca"
   },
   "outputs": [],
   "source": [
    "# Install dependencies (this is mostly for Google Colab, as the other dependences are available by default in Colab)\n",
    "try:\n",
    "  import datasets, evaluate, accelerate\n",
    "  import gradio as gr\n",
    "except ModuleNotFoundError:\n",
    "  !pip install -U datasets evaluate accelerate gradio # -U stands for \"upgrade\" so we'll get the latest version by default\n",
    "  import datasets, evaluate, accelerate\n",
    "  import gradio as gr\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(f\"Using transformers version: {transformers.__version__}\")\n",
    "print(f\"Using datasets version: {datasets.__version__}\")\n",
    "print(f\"Using torch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOTU0N7KJJfv"
   },
   "source": [
    "## GETTING DATASET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217,
     "referenced_widgets": [
      "dd43a336c3e54718988ff32422a6e982",
      "1e9c671023fa4e8ebaa82edc01dbc312",
      "45d48121fd454804b50b6efe812ec1e2",
      "81234042a52e4b76a4490438198ea12f",
      "f0296b19b0cc473d899078ccde8c659d",
      "f4110ecac51b436092da4495dff92f3e",
      "5116ca6461a1490fb61dbae8e90f1ce6",
      "e459663b52394632a7ba3554eafdf263",
      "28156c1a94994cad910419c996b9e720",
      "951055bf572049d4af4843e9b7f58981",
      "f333fff5cadf47579cbf922d99db8506",
      "f22db2ea3e0941dba561376693690e60",
      "32e26bdb0e944a2ca5bfbcb33a2e4443",
      "25a544db2d1a447e937df0c0e5305364",
      "3a834beea5a948ecbb1c4272f701e95d",
      "9a7a7a41a4da405087c992b5fc63987d",
      "9a9d8ef5422e40728999518b9316b242",
      "02304985b57a43a6b71e193c67e14e45",
      "da1030dd64834800a298b485bb55bd96",
      "d2050ab2256e45168dd72af87e76db73",
      "84270fc81f9c436c8865ae5da72170e9",
      "ea6f8858142e4b76b007fc3c4967d23f",
      "d80256ce79404684b58b408e3b971170",
      "728e779da6b4463396010abd03a6e37d",
      "8f79e41330bf44bd832e6b0260d0254e",
      "a5f726d005ac4ff2b71bb9dc1d91341a",
      "86f784f303e74c8f89d44e22f99b067b",
      "ac2e154bcf7449e0a59b4d253de279cc",
      "53279e3093624729a537314cea2539a3",
      "6d45659a6bdd437e81c9c35e86a73c0b",
      "068c55cbdc4f4117bf76692b721ebacb",
      "66715ca3cfd0411fa8848094c7be3727",
      "5d6820cbbaec4012a5755e04ceebf4d5"
     ]
    },
    "id": "ULXbtq2cYzGb",
    "outputId": "7770d66c-0928-402c-ae4d-43d1f72d3679"
   },
   "outputs": [],
   "source": [
    "# Load the dataset from Hugging Face Hub\n",
    "dataset = datasets.load_dataset(path=\"mrdbourke/learn_hf_food_not_food_image_captions\")\n",
    "\n",
    "# Inspect the dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_7C0Pj5gsEi",
    "outputId": "8ca5af1a-a2f9-4b2c-f502-42be8374864a"
   },
   "outputs": [],
   "source": [
    "# What features are there?\n",
    "dataset.column_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gD7I1v1ygzar",
    "outputId": "0cc2b959-04ea-4f70-e1d6-489ce42d221a"
   },
   "outputs": [],
   "source": [
    "# Access the training split\n",
    "dataset[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VspEe9q6g8vz",
    "outputId": "70d31b09-7a8f-40c0-805c-7e4f4fb3eb2e"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random_indexs = random.sample(range(len(dataset[\"train\"])), 5)\n",
    "random_samples = dataset[\"train\"][random_indexs]\n",
    "\n",
    "print(f\"[INFO] Random samples from dataset:\\n\")\n",
    "for item in zip(random_samples[\"text\"], random_samples[\"label\"]):\n",
    "    print(f\"Text: {item[0]} | Label: {item[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQybNiYuhJVv",
    "outputId": "230ec2dc-320b-4cd4-cea6-daca541e0af4"
   },
   "outputs": [],
   "source": [
    "# Get unique label values\n",
    "dataset[\"train\"].unique(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VMeHwfjChPFd",
    "outputId": "0385bd53-dcf8-4d75-816b-1b6b96ca0967"
   },
   "outputs": [],
   "source": [
    "# Check number of each label\n",
    "from collections import Counter\n",
    "\n",
    "Counter(dataset[\"train\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "pOlUYRvOhba-",
    "outputId": "5ef37d9b-3003-42f9-e88b-f27392b34dae"
   },
   "outputs": [],
   "source": [
    "# Turn our dataset into a DataFrame and get a random sample\n",
    "food_not_food_df = pd.DataFrame(dataset[\"train\"])\n",
    "food_not_food_df.sample(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "GwX84oNnhpt8",
    "outputId": "278bb594-fcdc-4087-d183-c06688ee5b8d"
   },
   "outputs": [],
   "source": [
    "# Get the value counts of the label column\n",
    "food_not_food_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWIzrmjPmDe3"
   },
   "source": [
    "# Preparing data for text classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qNni52jEmMQ-",
    "outputId": "f2ffe32e-a594-4af9-8169-c7bb4c574d1c"
   },
   "outputs": [],
   "source": [
    "# Create mapping from id2label and label2id\n",
    "id2label = {0: \"not_food\", 1: \"food\"}\n",
    "label2id = {\"not_food\": 0, \"food\": 1}\n",
    "\n",
    "print(f\"Label to ID mapping: {label2id}\")\n",
    "print(f\"ID to Label mapping: {id2label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "efXOYumemSKn",
    "outputId": "bb443bf7-fd6c-4628-91ec-8f4f08add806"
   },
   "outputs": [],
   "source": [
    "# Create mappings programmatically from dataset\n",
    "id2label = {idx: label for idx, label in enumerate(dataset[\"train\"].unique(\"label\")[::-1])} # reverse sort list to have \"not_food\" first\n",
    "label2id = {label: idx for idx, label in id2label.items()}\n",
    "\n",
    "print(f\"Label to ID mapping: {label2id}\")\n",
    "print(f\"ID to Label mapping: {id2label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tb4d6_Q9mX7I",
    "outputId": "54ff65e0-af07-4acd-ce2b-c9dd981e486f"
   },
   "outputs": [],
   "source": [
    "# Turn labels into 0 or 1 (e.g. 0 for \"not_food\", 1 for \"food\")\n",
    "def map_labels_to_number(example):\n",
    "  example[\"label\"] = label2id[example[\"label\"]]\n",
    "  return example\n",
    "\n",
    "example_sample = {\"text\": \"This is a sentence about my favourite food: honey.\", \"label\": \"food\"}\n",
    "\n",
    "# Test the function\n",
    "map_labels_to_number(example_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153,
     "referenced_widgets": [
      "b6e9e20b037a484f857969a1637d3a74",
      "dd3a37cd172445f1b3147537f39ad8cd",
      "e001a11e87fb4d9bb3b59d814da8c5f2",
      "ad3dda7972584ac989ccc3674eabcef6",
      "293ab4f3e3ad4d028a0f4c6a9bc5c062",
      "19e52b5033f54ba894d24e317ea8e50f",
      "3b8fea7478a3488cb044ce709b13c21c",
      "35c703b0dbc34d15be5c56953ccd4f0c",
      "1ed9b8095aac412c8757b120b715b411",
      "464840646bfa4d75a353753a5a6d82c7",
      "17800d78b13048c9a788b96eef5dbaf0"
     ]
    },
    "id": "zpU1NFaimidB",
    "outputId": "19e350e5-d670-45c4-f823-a3b44a3ab6e5"
   },
   "outputs": [],
   "source": [
    "# Map our dataset labels to numbers\n",
    "dataset = dataset[\"train\"].map(map_labels_to_number)\n",
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0nhuYH4Kmn_Q",
    "outputId": "c3e3f134-7dee-421e-8a07-c18ba7ade79f"
   },
   "outputs": [],
   "source": [
    "# Shuffle the dataset and view the first 5 samples (will return different results each time)\n",
    "dataset.shuffle()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O09C5uRMmpo1",
    "outputId": "13d6cd2e-1925-47ce-f052-193574ac505b"
   },
   "outputs": [],
   "source": [
    "# Create train/test splits\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42) # note: seed isn't needed, just here for reproducibility, without it you will get different splits each time you run the cell\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AoSutHBzm9Z4",
    "outputId": "fa30436e-4572-4395-c5d8-679d81ba37e4"
   },
   "outputs": [],
   "source": [
    "random_idx_train = random.randint(0, len(dataset[\"train\"]))\n",
    "random_sample_train = dataset[\"train\"][random_idx_train]\n",
    "\n",
    "random_idx_test = random.randint(0, len(dataset[\"test\"]))\n",
    "random_sample_test = dataset[\"test\"][random_idx_test]\n",
    "\n",
    "print(f\"[INFO] Random sample from training dataset:\")\n",
    "print(f\"Text: {random_sample_train['text']}\\nLabel: {random_sample_train['label']} ({id2label[random_sample_train['label']]})\\n\")\n",
    "print(f\"[INFO] Random sample from testing dataset:\")\n",
    "print(f\"Text: {random_sample_test['text']}\\nLabel: {random_sample_test['label']} ({id2label[random_sample_test['label']]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mzArwTZoVjz"
   },
   "source": [
    "# Tokenizing text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319,
     "referenced_widgets": [
      "8f90c4bef34a4d4a806e3171723fb3d8",
      "70065be9dc0b44efa17888c3ffe1a0f9",
      "8ee93fb508734ca7a114b5a5187a4741",
      "4c7e51d540c4468888dc2d96da40b492",
      "7f75138f7902459293da67381c8add61",
      "fcd54237e7054a42992a0d5c3bebd80c",
      "c8560a98e0ef4efeb719f2d5caa01e22",
      "9bb541bdad394a5c8adafacb855c3134",
      "f3b394da49e945e3b6a2dbbeb1b32f19",
      "8a74741bceb34566ad4959993244494d",
      "19a0370fa5c24b5c82a48ddc625dcde9",
      "f375d37853c34a3eac88432838192c80",
      "c0d0c90e1ed445c09027b5574dead267",
      "64e4e2ef18e04a809dd75c63ebe67ea3",
      "cae28d25c22742fd9f86dbe483e51a6f",
      "274aef50d6a545ac832ccfdb00f79f9f",
      "09c0e5e97a604600ae940ff59313802b",
      "65c8331179124b8082e1fcae9befdb56",
      "56157c3d20664b37b0e1f0beae41f8c7",
      "81a0a1c8abe14085b11fc1f83162562e",
      "5ee79b2cf1cc4e8694ab2941c08c1eb2",
      "6e06f83e49d141a4b13de275d3124210",
      "ee4c97f17441460c945deee2c989a8ca",
      "db5fd5a7e96e44ce973b4fe76c134486",
      "1fd4d2fb93b0447aaad39841ac1bda57",
      "1a980db058224b6784384d7afa5f2586",
      "2c47cbb72c1c46baabc2784d78616a05",
      "d3b7050dbd5249439af32fc36d4fd58d",
      "8f03508cd7504764ba38912fb2bcacf4",
      "5c276a0a8cb2415dbcfc5b6c61b8ab63",
      "72dda18eab514ee5ace50ddf8cddc93b",
      "e493399718d84bbdb1e1271898555e3e",
      "57dc0b2541fc4ef5b0af9d17294dd91e",
      "2f7482c6c27645fb8fb72573cd76d2b2",
      "5590464f4a464ea18711d299d4fec623",
      "fe7893304e374341969e2dda9cb9e8b1",
      "109f63faee334ab8bac4cea41ac79455",
      "fa3161d0dc2442b3af9e443c71737486",
      "2329b1d219a44609ad0951523385768c",
      "0dd0e91001064010a0d5b94328b559a5",
      "462a7eb09afd448785a5ce851e898a3f",
      "eadaa490d81c4f86a080a939da2e7d6c",
      "dd339f8c1658440dad8db784dc386168",
      "f29b99c350e347c798e98f7feb676f04"
     ]
    },
    "id": "C6oBZuAsoXPw",
    "outputId": "c639d2b7-93a4-4542-a98e-289c65af2012"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"distilbert/distilbert-base-uncased\",\n",
    "                                          use_fast=True) # uses fast tokenization (backed by tokenziers library and implemented in Rust) by default, if not available will default to Python implementation\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cvpq2CsUeKpw",
    "outputId": "69723115-5ca3-4166-e94c-380c31050cab"
   },
   "outputs": [],
   "source": [
    "# Test out tokenizer\n",
    "tokenizer(\"I love pizza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CE928cPjf4Ye",
    "outputId": "3d2fe2f1-8373-40dd-8a65-05ed23e22f43"
   },
   "outputs": [],
   "source": [
    "# Try adding a \"!\" at the end\n",
    "tokenizer(\"I love pizza!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-aYJz80Af8Ku",
    "outputId": "d35178cc-fb86-41de-a5b8-543a2df1a5a9"
   },
   "outputs": [],
   "source": [
    "# Get the length of the vocabulary\n",
    "length_of_tokenizer_vocab = len(tokenizer.vocab)\n",
    "print(f\"Length of tokenizer vocabulary: {length_of_tokenizer_vocab}\")\n",
    "\n",
    "# Get the maximum sequence length the tokenizer can handle\n",
    "max_tokenizer_input_sequence_length = tokenizer.model_max_length\n",
    "print(f\"Max tokenizer input sequence length: {max_tokenizer_input_sequence_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6YxOrAsrilBm",
    "outputId": "c3d5baa2-272a-4ae8-a2dc-437edd44a9ac"
   },
   "outputs": [],
   "source": [
    "# Does \"daniel\" occur in the vocab?\n",
    "tokenizer.vocab[\"daniel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y8wmz_xXip1-",
    "outputId": "710754b2-5ac4-4a68-acf5-e15b67309980"
   },
   "outputs": [],
   "source": [
    "tokenizer.vocab[\"pizza\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vOiBr7SJjYCU",
    "outputId": "ebbb94c8-3871-43f5-8466-ec304b0a844b"
   },
   "outputs": [],
   "source": [
    "# Get the length of the vocabulary\n",
    "length_of_tokenizer_vocab = len(tokenizer.vocab)\n",
    "print(f\"Length of tokenizer vocabulary: {length_of_tokenizer_vocab}\")\n",
    "\n",
    "# Get the maximum sequence length the tokenizer can handle\n",
    "max_tokenizer_input_sequence_length = tokenizer.model_max_length\n",
    "print(f\"Max tokenizer input sequence length: {max_tokenizer_input_sequence_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GKqH7qELnSFg",
    "outputId": "cda81183-7196-4554-99ed-6b6a0997639c"
   },
   "outputs": [],
   "source": [
    "# Does \"daniel\" occur in the vocab?\n",
    "tokenizer.vocab[\"daniel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h6alnmICnWAa",
    "outputId": "055d0519-4ba4-42f6-97e0-8486ef884db0"
   },
   "outputs": [],
   "source": [
    "tokenizer.vocab[\"pizza\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uV6QEIuFna8M",
    "outputId": "fcc4749e-abe7-4207-c64a-056c6cba380c"
   },
   "outputs": [],
   "source": [
    "tokenizer(\"akash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GaxoOQONnqJQ",
    "outputId": "ff15725d-e569-4c53-e053-7efd2403562a"
   },
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer(\"akash\").input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y3Fg81-NnssO",
    "outputId": "ba36a860-7850-4f76-dd68-463245809fc2"
   },
   "outputs": [],
   "source": [
    "# Try to tokenize an emoji\n",
    "tokenizer.convert_ids_to_tokens(tokenizer(\"ðŸ•\").input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lSCldedEnw6l",
    "outputId": "7cd319c8-1b17-4255-db9e-59b8d2ca50ca"
   },
   "outputs": [],
   "source": [
    "# Get the first 5 items in the tokenizer vocab\n",
    "sorted(tokenizer.vocab.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBsPAS8an5xx",
    "outputId": "d7b6eec8-495d-46c2-94d0-62873121ec33"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.sample(sorted(tokenizer.vocab.items()), k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mK2wPW6moCg8"
   },
   "source": [
    "# Making a preprocessing function to tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NVNlTPEoI6Y"
   },
   "outputs": [],
   "source": [
    "def tokenize_text(examples):\n",
    "    \"\"\"\n",
    "    Tokenize given example text and return the tokenized text.\n",
    "    \"\"\"\n",
    "    return tokenizer(examples[\"text\"],\n",
    "                     padding=True, # pad short sequences to longest sequence in the batch\n",
    "                     truncation=True) # truncate long sequences to the maximum length the model can handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CG8f1rMVoQ-m",
    "outputId": "093789b0-613b-4ec0-a5b2-61c612d85b6f"
   },
   "outputs": [],
   "source": [
    "example_sample_2 = {\"text\": \"I love pizza\", \"label\": 1}\n",
    "\n",
    "# Test the function\n",
    "tokenize_text(example_sample_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255,
     "referenced_widgets": [
      "024b9e9055f44d1e97618c5540008347",
      "0832ba7473aa4d2a8da96d5a01cec246",
      "be768fb4051f4b2db9cea25bbd1eda8e",
      "af8981dce80f4f1b8b4beaf9978df20c",
      "e6b5f31a28414495b5e02ef93f936a32",
      "59a752d4cb5344f2b82210e75f4d181d",
      "d336086c649e4f8da2b988b2d7de8890",
      "540a4a3a4a814ae291bfb4d7712bcf87",
      "3bc8537643134459ba794c659886a7ea",
      "2a147514fa7843cfb1c2dc5099275393",
      "d2330fec1e2247ea91c3ad2158199793",
      "17a323aae6bd42a39ee718d81119247b",
      "b7af2ebaab5f467ba619f90c0cccd535",
      "17ee1387c1bf4959b4d1efc5bf60aedf",
      "615a29aae8bc4035a4de5423914fdf5a",
      "16a0e63ae87b4292908a4c1b5f9388f7",
      "e31e511048d94201b18ebc99ce289c4c",
      "a986c38372534cc7a923acc068167d17",
      "11bcb68da4ff4ec1a6a618fd0730ee71",
      "a91bf5923616461fb49097fd8218ee80",
      "96937d1cd0464880827479aa7bd3b563",
      "5becf6ae46824d6691317b65416c3684"
     ]
    },
    "id": "TpAXDuShoYCL",
    "outputId": "b2b5129b-b2ab-4d5d-d91f-feb0367f12b3"
   },
   "outputs": [],
   "source": [
    "# Map our tokenize_text function to the dataset\n",
    "tokenized_dataset = dataset.map(function=tokenize_text,\n",
    "                                batched=True, # set batched=True to operate across batches of examples rather than only single examples\n",
    "                                batch_size=1000) # defaults to 1000, can be increased if you have a large dataset\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KB0etnbQodb5"
   },
   "outputs": [],
   "source": [
    "# Get two samples from the tokenized dataset\n",
    "train_tokenized_sample = tokenized_dataset[\"train\"][0]\n",
    "test_tokenized_sample = tokenized_dataset[\"test\"][0]\n",
    "\n",
    "for key in train_tokenized_sample.keys():\n",
    "    print(f\"[INFO] Key: {key}\")\n",
    "    print(f\"Train sample: {train_tokenized_sample[key]}\")\n",
    "    print(f\"Test sample: {test_tokenized_sample[key]}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loXdbF86onCI"
   },
   "source": [
    "# Setting up an evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "3ff52b4ece1a4596ba27a99293fa8549",
      "2c66029677b646c8a69812e3bc4e0734",
      "a192f775374f4a4abc837fa2a2e49882",
      "640518e29f1a498cbc063a9c5e6ea0f3",
      "5cd3d4cc5bd24fe282dde3484cfcedbb",
      "99129dfd1a83453e8b3a644ba110efdc",
      "51bed9ad7b074ffda0e1644ab37b0046",
      "a7df9a7ef08a4f409e472b9ab2e98b53",
      "d4147aeb74e246f5ab5fe884ee7af6e6",
      "fffda57231004873aa599fe40733184c",
      "289c8d81fb374bb8b6fc548b293c1c7d"
     ]
    },
    "id": "2W-iAeAhopb_",
    "outputId": "11a7d3ba-f465-4f95-aae7-fb5ea11c5f9c"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_accuracy(predictions_and_labels: Tuple[np.array, np.array]):\n",
    "  \"\"\"\n",
    "  Computes the accuracy of a model by comparing the predictions and labels.\n",
    "  \"\"\"\n",
    "  predictions, labels = predictions_and_labels\n",
    "\n",
    "  # Get highest prediction probability of each prediction if predictions are probabilities\n",
    "  if len(predictions.shape) >= 2:\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "  return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IfDwNTOEoxoQ",
    "outputId": "e8856726-9356-4de0-9c90-2f10126b70d8"
   },
   "outputs": [],
   "source": [
    "# Create example list of predictions and labels\n",
    "example_predictions_all_correct = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "example_predictions_one_wrong = np.array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
    "example_labels = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "# Test the function\n",
    "print(f\"Accuracy when all predictions are correct: {compute_accuracy((example_predictions_all_correct, example_labels))}\")\n",
    "print(f\"Accuracy when one prediction is wrong: {compute_accuracy((example_predictions_one_wrong, example_labels))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fDXbqdayo0Ep",
    "outputId": "f1bd3973-33b5-487e-ee74-cf98700bb74e"
   },
   "outputs": [],
   "source": [
    "# Get id and label mappings\n",
    "print(f\"id2label: {id2label}\")\n",
    "print(f\"label2id: {label2id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138,
     "referenced_widgets": [
      "0710fd13144e406eb23f0d72f22d85a6",
      "0983cdae8f384c97aa162f27ca7abe41",
      "6161a72bfcb642b39d9d5096cb888351",
      "17a6fa9e28574afbb98db9e332fb9d89",
      "92059239458746bcabcaf077e42a4a3f",
      "4009ace8cd8a413ea2ec0edd3441a73f",
      "eabe278e1b1c455484571ece7695b495",
      "3866d467617f4cc1be6076846970ce7a",
      "95555a2a7a9d4cc4b6827071cfb0fc97",
      "bc52739d9b9b4535bb4cb5b1372fcdf6",
      "747ed1adfafe4197ad53706c1edc818d"
     ]
    },
    "id": "gxh2Zu5yo5kO",
    "outputId": "f637ea6e-85ca-42c8-92af-75bedb376d6d"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Setup model for fine-tuning with classification head (top layers of network)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"distilbert/distilbert-base-uncased\",\n",
    "    num_labels=2, # can customize this to the number of classes in your dataset\n",
    "    id2label=id2label, # mappings from class IDs to the class labels (for classification tasks)\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "syrdWZ86pB6I",
    "outputId": "b6381d30-3163-4b91-8320-2715bfee33c3"
   },
   "outputs": [],
   "source": [
    "# Try and make a prediction with the loaded model (this will error)\n",
    "model(**tokenized_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OWrlJDsupH_M",
    "outputId": "77e8a3b6-e8f9-47bc-85be-b114cc8d5bac"
   },
   "outputs": [],
   "source": [
    "# Inspect the model\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAMse1oLpQBo"
   },
   "source": [
    "#  Counting the parameters of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhjESDe4pXBn",
    "outputId": "e4d9f4db-e719-45d6-e0b7-70310530769c"
   },
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    \"\"\"\n",
    "    Count the parameters of a PyTorch model.\n",
    "    \"\"\"\n",
    "    trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_parameters = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    return {\"trainable_parameters\": trainable_parameters, \"total_parameters\": total_parameters}\n",
    "\n",
    "# Count the parameters of the model\n",
    "count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EKZHU-mRpakI",
    "outputId": "23161e03-8182-48d0-ea79-af01f4e87313"
   },
   "outputs": [],
   "source": [
    "# Create model output directory\n",
    "from pathlib import Path\n",
    "\n",
    "# Create models directory\n",
    "models_dir = Path(\"models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create model save name\n",
    "model_save_name = \"learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n",
    "\n",
    "# Create model save path\n",
    "model_save_dir = Path(models_dir, model_save_name)\n",
    "\n",
    "model_save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9C5YyFLWpgsZ",
    "outputId": "7ac36206-f19f-4837-cc2b-aa0bca71832f"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "print(f\"[INFO] Saving model checkpoints to: {model_save_dir}\")\n",
    "\n",
    "# Create training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_save_dir,\n",
    "    learning_rate=0.0001,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\", # was previously \"evaluation_strategy\"\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3, # limit the total amount of save checkpoints (so we don't save num_epochs checkpoints)\n",
    "    use_cpu=False, # set to False by default, will use CUDA GPU or MPS device if available\n",
    "    seed=42, # set to 42 by default for reproducibility\n",
    "    load_best_model_at_end=True, # load the best model when finished training\n",
    "    logging_strategy=\"epoch\", # log training results every epoch\n",
    "    report_to=\"none\", # optional: log experiments to Weights & Biases/other similar experimenting tracking services (we'll turn this off for now)\n",
    "    # push_to_hub=True # optional: automatically upload the model to the Hub (we'll do this manually later on)\n",
    "    # hub_token=\"your_token_here\" # optional: add your Hugging Face Hub token to push to the Hub (will default to huggingface-cli login)\n",
    "    hub_private_repo=False # optional: make the uploaded model private (defaults to False)\n",
    ")\n",
    "\n",
    "# Optional: Print out training_args to inspect (warning, it is quite a long output)\n",
    "# training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qNGZQ2Kvpmoe",
    "outputId": "908dba9b-0093-4c94-e0f0-e5dae549080a"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Setup Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    # Note: the 'tokenizer' parameter will be changed to 'processing_class' in Transformers v5.0.0\n",
    "    tokenizer=tokenizer, # Pass tokenizer to the Trainer for dynamic padding (padding as the training happens) (see \"data_collator\" in the Trainer docs)\n",
    "    compute_metrics=compute_accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "PT1XqY9c_k8f",
    "outputId": "db8f3444-3931-4512-a230-c269a30706ae"
   },
   "outputs": [],
   "source": [
    "# Train a text classification model\n",
    "results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRWVHCKAAKUw"
   },
   "outputs": [],
   "source": [
    "# Inspect training metrics\n",
    "for key, value in results.metrics.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "riYssLpvAUaA",
    "outputId": "05a1065c-7929-4fb3-ea96-93782abfcb80"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "print(f\"[INFO] Saving model to {model_save_dir}\")\n",
    "trainer.save_model(output_dir=model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k4xf_PNJAYEF",
    "outputId": "66d03102-7486-4ab1-a49b-63c7d362587f"
   },
   "outputs": [],
   "source": [
    "# Get training history\n",
    "trainer_history_all = trainer.state.log_history\n",
    "trainer_history_metrics = trainer_history_all[:-1] # get everything except the training time metrics (we've seen these already)\n",
    "trainer_history_training_time = trainer_history_all[-1] # this is the same value as results.metrics from above\n",
    "\n",
    "# View the first 4 metrics from the training history\n",
    "trainer_history_metrics[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "esrW2JaQAY6t",
    "outputId": "07ce8ef2-19f2-49a6-a12c-55c8542306ca"
   },
   "outputs": [],
   "source": [
    "import pprint # import pretty print for nice printing of lists\n",
    "\n",
    "# Extract training and evaluation metrics\n",
    "trainer_history_training_set = []\n",
    "trainer_history_eval_set = []\n",
    "\n",
    "# Loop through metrics and filter for training and eval metrics\n",
    "for item in trainer_history_metrics:\n",
    "    item_keys = list(item.keys())\n",
    "    # Check to see if \"eval\" is in the keys of the item\n",
    "    if any(\"eval\" in item for item in item_keys):\n",
    "        trainer_history_eval_set.append(item)\n",
    "    else:\n",
    "        trainer_history_training_set.append(item)\n",
    "\n",
    "# Show the first two items in each metric set\n",
    "print(f\"[INFO] First two items in training set:\")\n",
    "pprint.pprint(trainer_history_training_set[:2])\n",
    "\n",
    "print(f\"\\n[INFO] First two items in evaluation set:\")\n",
    "pprint.pprint(trainer_history_eval_set[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "G9tt-ogTAeLA",
    "outputId": "73f771c5-e243-438f-8d31-c7c4ea196fe4"
   },
   "outputs": [],
   "source": [
    "# Create pandas DataFrames for the training and evaluation metrics\n",
    "trainer_history_training_df = pd.DataFrame(trainer_history_training_set)\n",
    "trainer_history_eval_df = pd.DataFrame(trainer_history_eval_set)\n",
    "\n",
    "trainer_history_training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "U0QM4pbvAgLn",
    "outputId": "d0963cff-7f69-43bf-c878-bd96e9ce0322"
   },
   "outputs": [],
   "source": [
    "# Plot training and evaluation loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(trainer_history_training_df[\"epoch\"], trainer_history_training_df[\"loss\"], label=\"Training loss\")\n",
    "plt.plot(trainer_history_eval_df[\"epoch\"], trainer_history_eval_df[\"eval_loss\"], label=\"Evaluation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Text classification with DistilBert training and evaluation loss over time\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130,
     "referenced_widgets": [
      "ac0544af7a82436e903c70b3c587dc6b",
      "be07231e3f134cc5a8856e00c7826f18",
      "c4b3f91ec19d4631a5b93b1b46424ef3",
      "29fc639e904442ed84cca8302ceb49d4",
      "c4b66f694aca4ab2b0c3da621c216d67",
      "22327b9bc8a1415bb8a953d3e0c19993",
      "cadd438e83b84f9aa5d4e055a1432f73",
      "9133791d0373474fa89cf00987c472eb",
      "8073de0ad4d7420b9606920d4e904657",
      "d412ab91538949d79a9fe295381ca8f4",
      "b44dc06334954fa890d4479b2d6805f4",
      "1a81eec46f4542aba3fb3f0d9a0a31b8",
      "4a9e4a814b1e40329020d200e8665022",
      "c897a41e1934409f8a4fd6ae8ca3b594",
      "a2012bd282334496bfc3e480f5b52697",
      "cfc875c907a949e8a8886fe4b9ada7ec",
      "f12368aba4f14f9cb1e8c84b59ed97f5",
      "5187ef9eb72e4273becfa943ed07cd74",
      "d8e1cf02711743698693106dbe4322b9",
      "62d206903646470db00c64cc8085b05f",
      "1120252740fb46f1bff809d56ac8ee46",
      "e6b222c2e5e34e6ea2339673d89791e1",
      "9b72431f43e34b229fbdf7dfd65e0832",
      "ee527d8003774417a175c93b4530e3a4",
      "39e3c3dc012a4d83abd209d6b853053f",
      "f51ca33b267f49b6bd48194274f47509",
      "1c9fd100a9384e3bb4db123381dcfde7",
      "d109ca3f410c42368b3dcb6cdd0747d4",
      "b1f197437b4d4fb2b5db0368ba982e12",
      "648c17a5fe314310a756800f047fea6d",
      "b363811d8f92409e92d51d884f3ac99c",
      "80ab14d78ac6411db1451abbc1da8675",
      "fa3f41837cbd4a9da0a27861e013108c"
     ]
    },
    "id": "r3y8fn28AjER",
    "outputId": "8e8fc28e-18a3-45d8-b3a3-a5ac32dfe472"
   },
   "outputs": [],
   "source": [
    "# Save our model to the Hugging Face Hub\n",
    "# This will be public, since we set hub_private_repo=False in our TrainingArguments\n",
    "model_upload_url = trainer.push_to_hub(\n",
    "    commit_message=\"Uploading food not food text classifier model\",\n",
    "    # token=\"YOUR_HF_TOKEN_HERE\" # This will default to the token you have saved in your Hugging Face config\n",
    ")\n",
    "print(f\"[INFO] Model successfully uploaded to Hugging Face Hub with at URL: {model_upload_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "lHG56QVHAnNz",
    "outputId": "585a1f8e-e0c6-44a2-e26c-91c0ea69fea4"
   },
   "outputs": [],
   "source": [
    "# Perform predictions on the test set\n",
    "predictions_all = trainer.predict(tokenized_dataset[\"test\"])\n",
    "prediction_values = predictions_all.predictions\n",
    "prediction_metrics = predictions_all.metrics\n",
    "\n",
    "print(f\"[INFO] Prediction metrics on the test data:\")\n",
    "prediction_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWo8XZCxAtv-",
    "outputId": "6f32b36c-984c-4110-bd00-d772e975cd0f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Get prediction probabilities (this is optional, could get the same results with step 2 onwards)\n",
    "pred_probs = torch.softmax(torch.tensor(prediction_values), dim=1)\n",
    "\n",
    "# 2. Get the predicted labels\n",
    "pred_labels = torch.argmax(pred_probs, dim=1)\n",
    "\n",
    "# 3. Get the true labels\n",
    "true_labels = dataset[\"test\"][\"label\"]\n",
    "\n",
    "# 4. Compare predicted labels to true labels to get the test accuracy\n",
    "test_accuracy = accuracy_score(y_true=true_labels,\n",
    "                               y_pred=pred_labels)\n",
    "\n",
    "print(f\"[INFO] Test accuracy: {test_accuracy*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "iSBMl6nWAyTT",
    "outputId": "1fd03bfd-781b-42d6-bf3e-c95ede087b20"
   },
   "outputs": [],
   "source": [
    "# Make a DataFrame of test predictions\n",
    "test_predictions_df = pd.DataFrame({\n",
    "    \"text\": dataset[\"test\"][\"text\"],\n",
    "    \"true_label\": true_labels,\n",
    "    \"pred_label\": pred_labels,\n",
    "    \"pred_prob\": torch.max(pred_probs, dim=1).values\n",
    "})\n",
    "\n",
    "test_predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "qh2kCMSDA0d3",
    "outputId": "fe2ec487-7df1-4ffb-deaa-58696197b3cc"
   },
   "outputs": [],
   "source": [
    "# Show 10 examples with low prediction probability\n",
    "test_predictions_df.sort_values(\"pred_prob\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OtWIsqf0A3yl"
   },
   "outputs": [],
   "source": [
    "# Setup local model path\n",
    "local_model_path = \"models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n",
    "\n",
    "# Setup Hugging Face model path (see: https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased)\n",
    "# Note: Be sure to change \"mrdbourke\" to your own Hugging Face username\n",
    "huggingface_model_path = \"mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NhXP62uVA8xY",
    "outputId": "2bd18a21-50e7-47d4-8ec6-a720aeec201a"
   },
   "outputs": [],
   "source": [
    "def set_device():\n",
    "    \"\"\"\n",
    "    Set device to CUDA if available, else MPS (Mac), else CPU.\n",
    "\n",
    "    This defaults to using the best available device (usually).\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n",
    "\n",
    "DEVICE = set_device()\n",
    "print(f\"[INFO] Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wT1UbFXMBAFB",
    "outputId": "156aa240-ff7e-43e0-c791-9fa98bdcd4e8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Set the batch size for predictions\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create an instance of transformers.pipeline\n",
    "food_not_food_classifier = pipeline(task=\"text-classification\", # we can use this because our model is an instance of AutoModelForSequenceClassification\n",
    "                                    model=local_model_path, # could also pass in huggingface_model_path\n",
    "                                    device=DEVICE, # set the target device\n",
    "                                    top_k=1, # only return the top predicted value\n",
    "                                    batch_size=BATCH_SIZE) # perform predictions on up to BATCH_SIZE number of samples at a time\n",
    "\n",
    "food_not_food_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x6359c7PBCx0",
    "outputId": "bad61ea6-d921-4d18-c879-6148cc7dec9c"
   },
   "outputs": [],
   "source": [
    "# Test our trained model on some example text\n",
    "sample_text_food = \"A delicious photo of a plate of scrambled eggs, bacon and toast\"\n",
    "food_not_food_classifier(sample_text_food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ia9Zr7JVBEsc",
    "outputId": "aeb095f4-5ce4-4f32-fd94-b2d72a9e77c4"
   },
   "outputs": [],
   "source": [
    "# Test the model on some more example text\n",
    "sample_text_not_food = \"A yellow tractor driving over the hill\"\n",
    "food_not_food_classifier(sample_text_not_food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcZrMw1ABJ35",
    "outputId": "0ea1fc21-b71b-4cbd-f930-e27c6fa81d0d"
   },
   "outputs": [],
   "source": [
    "# Pass in random text to the model\n",
    "food_not_food_classifier(\"cvnhertiejhwgdjshdfgh394587\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244,
     "referenced_widgets": [
      "faf3895f56e34f57be7d9b320eea8880",
      "b42b430bf044460ca2c03806e72faa9e",
      "3f6933bf0d95410f87ac8499d3d454ac",
      "abd6d6c50d184a46bde06554712ce02e",
      "e33bd15fa51d46e795de73141368185f",
      "980c40cb519a42168ef445b8a501802a",
      "3af58f5adc854f278da5cb2f06e22d98",
      "39e0e09296724791bf1b4e782a122c52",
      "74e82aef3b494b4099484c98b5737012",
      "96795c70634e43159e43cf58c8422d48",
      "b52b2264ad534cb4a2891ae9d3cce87a",
      "098147bca06f41b6b20b7365e1e65cb3",
      "24f6b4a3b7424879bc6fadaa611e642d",
      "c6c1e496d3a74ae1bdb7f55bee24de92",
      "b85a7e431666414f979821d8be3c41da",
      "613796384278443d88dd9e66e427eaa8",
      "070dc0d42d3145928fdd228e780a4607",
      "04a00bbb6739472f91673530ad8b9645",
      "ebc6285e3a544408a7c50f09b3f87c77",
      "45f05e42bf924e698465dbe3d7dec8d4",
      "d5b6092ebf1e4238b056f147f26eda2d",
      "0dc9cbbda0e14a62868c10c2f6a9df42",
      "58caf053313242a3b8a939b4ae3702e0",
      "398d415293b940baa720fb1b75b3ec53",
      "8267e0900e3240faa243f3b83ab0dedb",
      "5b0fda78d5124a6b836f3a922f160944",
      "154a89fc2a4e4b3a892b59cd8d3f1072",
      "7454e89feb4449ef97b07a3d24a531a4",
      "9c5804c6c4b747229587d030a4996847",
      "eaabf226875946c49cb7980376382cec",
      "32f9d6ca4113470b907d821a9dcb8142",
      "33ebf403349740498f228937714a6f21",
      "fe06d1e6f3074f17bacf23cc2f8e3c30",
      "78543b3f44f6434fa226a973ad5faad2",
      "fa1610f2898347da9c79fb26c8324a6a",
      "8820a8e1ad2749d795c04d0d507cf9aa",
      "7f2202b55e7f4122839e307961012f2c",
      "62fd0f105daa436c8e57c9c8513fb0c2",
      "2fbed24c4ab246588a091b9e17198114",
      "06a0d4b576bc4184bf19a33b0e72a718",
      "932efdc11628469fa19b33eb43f869a1",
      "fd3c426c29df4b0db31c704bb33223ec",
      "538577f1fed54a04adbb1032c7838d5f",
      "06c72b179f7949cb9af454bbf9ff64ff",
      "4307a20023334c48828df1514d1972a5",
      "9b4e321f96df4ceab4ba688a98141973",
      "1aacbbbd9cfb4965a744f65ce0d4fc0a",
      "be574e2462b24dc3ad85f55f751055e5",
      "e04405778ac64c72826395b1e4bc934e",
      "32b85e54d2174cec81c293df051daa46",
      "dbba4a35cf684809aed8b6ebbd8d85d1",
      "2a7b786b8fb944a2803393dde4448d72",
      "72f260f06bcd4742bcfa454aa38aa7f1",
      "bff30dceccf642989167496f74ed8ebd",
      "b93bb6cf87d549aa9603270b225e9f32",
      "dfa951b6d85843c4b4d512e672633e8a",
      "7d9be9aa3c144c21b1591922e8047325",
      "2c3b410226f5464692c8536a9afeb54c",
      "387bbde03d04406c80c09c2356778c27",
      "1c83a8d1b44542e4b7e65155e258b564",
      "61004971d4b94d85bde0e6789ba2ccb9",
      "9947f772e46a454b890a53c4b57d5da1",
      "e3fadd1228574980a02d0d9b592e56bb",
      "607fa6fe9bc247bba08f27dc6ba9ecb7",
      "9a00a987b74f4bc5bd5157f237ebcf41",
      "b366c8fe207f4f02a3d375e7b2f674eb"
     ]
    },
    "id": "WggmTi97BPGx",
    "outputId": "99abebf7-e424-41e5-859b-7d53d06ccf02"
   },
   "outputs": [],
   "source": [
    "# Pipeline also works with remote models (will have to laod the model locally first)\n",
    "food_not_food_classifier_remote = pipeline(task=\"text-classification\",\n",
    "                                           model=huggingface_model_path, # load the model from Hugging Face Hub (will download the model if it doesn't already exist)\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           device=DEVICE)\n",
    "\n",
    "food_not_food_classifier_remote(\"This is some new text about bananas and pancakes and ice cream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OTcDTjc2C75O",
    "outputId": "cde55ff4-802b-415a-c229-e80f7e864836"
   },
   "outputs": [],
   "source": [
    "# Create a list of sentences to make predictions on\n",
    "sentences = [\n",
    "    \"I whipped up a fresh batch of code, but it seems to have a syntax error.\",\n",
    "    \"We need to marinate these ideas overnight before presenting them to the client.\",\n",
    "    \"The new software is definitely a spicy upgrade, taking some time to get used to.\",\n",
    "    \"Her social media post was the perfect recipe for a viral sensation.\",\n",
    "    \"He served up a rebuttal full of facts, leaving his opponent speechless.\",\n",
    "    \"The team needs to simmer down a bit before tackling the next challenge.\",\n",
    "    \"The presentation was a delicious blend of humor and information, keeping the audience engaged.\",\n",
    "    \"A beautiful array of fake wax foods (shokuhin sampuru) in the front of a Japanese restaurant.\",\n",
    "    \"Daniel Bourke is really cool :D\",\n",
    "    \"My favoruite food is biltong!\"\n",
    "]\n",
    "\n",
    "food_not_food_classifier(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPYUCW4IC_3X",
    "outputId": "536d4476-8ce3-4177-f507-f112bb7ea6fd"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create 1000 sentences\n",
    "sentences_1000 = sentences * 100\n",
    "\n",
    "# Time how long it takes to make predictions on all sentences (one at a time)\n",
    "print(f\"[INFO] Number of sentences: {len(sentences_1000)}\")\n",
    "start_time_one_at_a_time = time.time()\n",
    "for sentence in sentences_1000:\n",
    "    # Make a prediction on each sentence one at a time\n",
    "    food_not_food_classifier(sentence)\n",
    "end_time_one_at_a_time = time.time()\n",
    "\n",
    "print(f\"[INFO] Time taken for one at a time prediction: {end_time_one_at_a_time - start_time_one_at_a_time} seconds\")\n",
    "print(f\"[INFO] Avg inference time per sentence: {(end_time_one_at_a_time - start_time_one_at_a_time) / len(sentences_1000)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "gFG6TA__DAkZ",
    "outputId": "b9c338ab-10e9-439a-f194-70a755333f8d"
   },
   "outputs": [],
   "source": [
    "for i in [10, 100, 1000, 10_000]:\n",
    "    sentences_big = sentences * i\n",
    "    print(f\"[INFO] Number of sentences: {len(sentences_big)}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Predict on all sentences in batches\n",
    "    food_not_food_classifier(sentences_big)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"[INFO] Inference time for {len(sentences_big)} sentences: {round(end_time - start_time, 5)} seconds.\")\n",
    "    print(f\"[INFO] Avg inference time per sentence: {round((end_time - start_time) / len(sentences_big), 8)} seconds.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "2hNWOUYMDEyo",
    "outputId": "2d8956fc-b648-4120-c2e2-e4a50c03d5e5"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Setup model path (can be local or on Hugging Face)\n",
    "# Note: Be sure to change \"mrdbourke\" to your own username\n",
    "model_path = \"mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n",
    "\n",
    "# Create an example to predict on\n",
    "sample_text_food = \"A delicious photo of a plate of scrambled eggs, bacon and toast\"\n",
    "\n",
    "# Prepare the tokenizer and tokenize the inputs\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_path)\n",
    "inputs = tokenizer(sample_text_food,\n",
    "                   return_tensors=\"pt\") # return the output as PyTorch tensors\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "WJ6gZj6CDK3P"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load our text classification model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3EQXX7d0DORD",
    "outputId": "c5e69266-1232-457b-eb76-a98b7789384c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs) # '**' means input all of the dictionary keys as arguments to the function\n",
    "    # outputs = model(input_ids=inputs[\"input_ids\"],\n",
    "    #                 attention_mask=inputs[\"attention_mask\"]) # same as above, but explicitly passing in the keys\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTYt4hBsDVX5"
   },
   "outputs": [],
   "source": [
    "# Get predicted class and prediction probability\n",
    "predicted_class_id = outputs.logits.argmax().item()\n",
    "prediction_probability = torch.softmax(outputs.logits, dim=1).max().item()\n",
    "\n",
    "print(f\"Text: {sample_text_food}\")\n",
    "print(f\"Predicted label: {model.config.id2label[predicted_class_id]}\")\n",
    "print(f\"Prediction probability: {prediction_probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-TdutrRDXmQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_path = \"mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_path)\n",
    "\n",
    "# Make sample text and tokenize it\n",
    "sample_text = \"A photo of a broccoli, salmon, rice and radish dish\"\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\")\n",
    "\n",
    "# Make a prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get predicted class and prediction probability\n",
    "output_logits = outputs.logits\n",
    "predicted_class_id = torch.argmax(output_logits, dim=1).item()\n",
    "predicted_class_label = model.config.id2label[predicted_class_id]\n",
    "predicted_probability = torch.softmax(output_logits, dim=1).max().item()\n",
    "\n",
    "# Print outputs\n",
    "print(f\"Text: {sample_text}\")\n",
    "print(f\"Predicted class: {predicted_class_label} (prob: {predicted_probability * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfVUPDh5F2o3"
   },
   "source": [
    "Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GrNSP36F1Mq"
   },
   "outputs": [],
   "source": [
    "print(f\"[INFO] Model training complete, saving model to local path: {model_save_dir}\")\n",
    "trainer.save_model(output_dir=model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0L5lYSA_F5NA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
